# D3A Algorithmic Fairness Hackathon

# Introduction
slides uploaded (first 10 mins intro slides)
text
aim of the exercise
fairness: group fairness and which groups are of interest

# Data
description in words
how to download R / Python

# Prediction
slip train test
define a ML for predicting outcome (binary classification)
confusion matrix
define metrics (F1 / AUPRC)
stratified metrics (output F1 / AUPRC for men vs women)

# Fairness
recap on confusion matrix
fairness metrics: Equalized Odds and Predictive Rate Parity are two most relevant (but there are more)
how to achieve fairness / mitigate bias
-> preprocessing
-> algorithmic tuning
-> postprocessing
https://journal.r-project.org/articles/RJ-2022-019/


# Groups
1. Improve the model (e.g., feature selection, from GLM to ML), but no fairness constraints
2. Improve the model: Pre-processing
3. Improve the model: Optimization
4. Improve the model: Post-processing

# Discussion
upload results to : xxxxxx google slides
results from performance metrics and fairness metrics









